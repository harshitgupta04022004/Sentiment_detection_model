{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10843911,"sourceType":"datasetVersion","datasetId":6734537}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"de928528-106d-47d2-98b3-18ba60c95989","cell_type":"code","source":"import tensorflow as tf\nfrom transformers import T5Tokenizer, TFT5ForConditionalGeneration\nimport pandas as pd\nimport re\nimport nltk\nimport numpy as np\n# import pandas as pd\nimport seaborn as sns\nimport sklearn as sk\nimport matplotlib.pyplot as plt\n\n# from nltk.tag import pos_tag\n# from nltk.corpus import wordnet\n# from nltk.corpus import stopwords\n# from nltk.stem import WordNetLemmatizer\n\n# from transformers import BertTokenizer\n# from tensorflow.keras.preprocessing.text import Tokenizer\n# from tensorflow.keras.preprocessing.sequence import pad_sequences\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Input, Bidirectional\n# from tensorflow.keras.callbacks import EarlyStopping\n# from tensorflow.keras.callbacks import ReduceLROnPlateau\n# from tensorflow.keras.regularizers import l2\n\n# from sklearn.preprocessing import OneHotEncoder\n# from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n\n# nltk downloads and stopwords\n# stop_words = stopwords.words(\"english\")\n# lemmatizer= WordNetLemmatizer()\n# nltk.download('wordnet')#, download_dir = '/root/nltk_data/')\n# nltk.download('omw-1.4')#, download_dir = '/root/nltk_data/')\n# # !unzip /root/nltk_data/corpora/wordnet.zip -d /root/nltk_data/corpora/\n# !unzip /root/nltk_data/corpora/omw-1.4.zip -d /root/nltk_data/corpora/","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:22:46.010813Z","iopub.execute_input":"2025-02-24T20:22:46.011153Z","iopub.status.idle":"2025-02-24T20:22:53.086366Z","shell.execute_reply.started":"2025-02-24T20:22:46.011127Z","shell.execute_reply":"2025-02-24T20:22:53.085476Z"}},"outputs":[],"execution_count":1},{"id":"f7daa6f7-90af-43e2-890a-72155d7c9b9f","cell_type":"code","source":"model_name = \"t5-small\"  \ntokenizer = T5Tokenizer.from_pretrained(model_name)\nmodel = TFT5ForConditionalGeneration.from_pretrained(model_name)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:22:53.087557Z","iopub.execute_input":"2025-02-24T20:22:53.088190Z","iopub.status.idle":"2025-02-24T20:22:55.317190Z","shell.execute_reply.started":"2025-02-24T20:22:53.088164Z","shell.execute_reply":"2025-02-24T20:22:55.316331Z"}},"outputs":[{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\nAll PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n\nAll the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","output_type":"stream"}],"execution_count":2},{"id":"0b202abd-fcb7-46db-a048-3e859b7bc97c","cell_type":"code","source":"import tensorflow as tf\nfrom transformers import TFT5ForConditionalGeneration\n\ndef shift_right(input_ids, pad_token_id, decoder_start_token_id):\n    \"\"\"\n    Shifts input ids to the right by one position.\n    The first token becomes the decoder_start_token_id.\n    \"\"\"\n    batch_size = tf.shape(input_ids)[0]\n    # Create a column vector for the start tokens.\n    start_tokens = tf.fill([batch_size, 1], decoder_start_token_id)\n    # Remove the last token and concatenate the start token at the beginning.\n    shifted_input_ids = tf.concat([start_tokens, input_ids[:, :-1]], axis=1)\n    return shifted_input_ids\n\nclass MultiTaskT5(tf.keras.Model):\n    def __init__(self, model_name, num_emotion_labels, num_sentiment_labels):\n        super(MultiTaskT5, self).__init__()\n        self.t5 = TFT5ForConditionalGeneration.from_pretrained(model_name)\n        self.dropout = tf.keras.layers.Dropout(0.1)\n        \n        # Classification heads for emotion and sentiment.\n        self.emotion_classifier = tf.keras.layers.Dense(\n            num_emotion_labels,\n            activation='softmax',\n            name=\"emotion_classifier\"\n        )\n        self.sentiment_classifier = tf.keras.layers.Dense(\n            num_sentiment_labels,\n            activation='softmax',\n            name=\"sentiment_classifier\"\n        )\n\n    def call(self, inputs, task, training=False, labels=None):\n        \"\"\"\n        Args:\n            inputs: Dictionary with keys 'input_ids' and 'attention_mask'.\n            task: One of 'emotion', 'sentiment', or 'summary'.\n            training: Boolean flag for training.\n            labels: (Optional) target sequence for summarization.\n        \"\"\"\n        if task in ['emotion', 'sentiment']:\n            # For classification, use only the encoder outputs.\n            encoder_outputs = self.t5.encoder(\n                input_ids=inputs['input_ids'],\n                attention_mask=inputs['attention_mask'],\n                training=training\n            )\n            # Mean-pool the encoder hidden states.\n            pooled_output = tf.reduce_mean(encoder_outputs.last_hidden_state, axis=1)\n            pooled_output = self.dropout(pooled_output, training=training)\n            if task == 'emotion':\n                return self.emotion_classifier(pooled_output)\n            else:  # task == 'sentiment'\n                return self.sentiment_classifier(pooled_output)\n\n        elif task == 'summary':\n            if labels is None:\n                # For inference, no labels are provided.\n                outputs = self.t5(\n                    input_ids=inputs['input_ids'],\n                    attention_mask=inputs['attention_mask'],\n                    training=training\n                )\n                return outputs.logits\n            else:\n                # For training, prepare decoder_input_ids by shifting labels to the right.\n                pad_token_id = self.t5.config.pad_token_id\n                decoder_start_token_id = self.t5.config.decoder_start_token_id\n                decoder_input_ids = shift_right(labels, pad_token_id, decoder_start_token_id)\n                outputs = self.t5(\n                    input_ids=inputs['input_ids'],\n                    attention_mask=inputs['attention_mask'],\n                    decoder_input_ids=decoder_input_ids,\n                    training=training\n                )\n                return outputs.logits\n        else:\n            raise ValueError(\"Unsupported task type. Use 'emotion', 'sentiment', or 'summary'.\")\n\n# Example usage:\n# Choose a T5 model size appropriate for your resource constraints.\nmodel_name = \"t5-small\"  \ntokenizer = T5Tokenizer.from_pretrained(model_name)\n\n# Instantiate the multi-task model (adjust label numbers as needed).\nmodel = MultiTaskT5(model_name=model_name, num_emotion_labels=6, num_sentiment_labels=3)\n\n# Example input dictionary (normally obtained via tokenizer):\n# inputs = tokenizer(\"summarize: Your input text here\", return_tensors=\"tf\")\n# For summarization, you might later use model.generate(...) for text generation.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:22:55.319296Z","iopub.execute_input":"2025-02-24T20:22:55.319552Z","iopub.status.idle":"2025-02-24T20:22:56.925398Z","shell.execute_reply.started":"2025-02-24T20:22:55.319527Z","shell.execute_reply":"2025-02-24T20:22:56.924500Z"}},"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n\nAll the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","output_type":"stream"}],"execution_count":3},{"id":"b289be28-5a5b-47d2-8d12-4df0591e8072","cell_type":"code","source":"# Assume emotion task has, for example, 6 labels and sentiment is binary\nnum_emotion_labels = 6  # Adjust this according to your dataset\nnum_sentiment_labels = 2\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:22:56.927124Z","iopub.execute_input":"2025-02-24T20:22:56.927374Z","iopub.status.idle":"2025-02-24T20:22:56.931395Z","shell.execute_reply.started":"2025-02-24T20:22:56.927352Z","shell.execute_reply":"2025-02-24T20:22:56.930339Z"}},"outputs":[],"execution_count":4},{"id":"edcc444c-722a-4253-b546-5c6b99346ff7","cell_type":"code","source":"\nmodel = MultiTaskT5( model_name, num_emotion_labels, num_sentiment_labels)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:22:56.932720Z","iopub.execute_input":"2025-02-24T20:22:56.933038Z","iopub.status.idle":"2025-02-24T20:22:57.920583Z","shell.execute_reply.started":"2025-02-24T20:22:56.933007Z","shell.execute_reply":"2025-02-24T20:22:57.919876Z"}},"outputs":[{"name":"stderr","text":"All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n\nAll the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n","output_type":"stream"}],"execution_count":5},{"id":"11573d32-b2ea-4ff5-9414-26cc025db857","cell_type":"code","source":"emotion_df = pd.read_csv(\"/kaggle/input/t5-fine-tune-for-sentiment-detection/train.txt\", sep=\";\", names=[\"text\", \"label\"], header=None)[:3000]\nsentiment_df = pd.read_csv(\"/kaggle/input/t5-fine-tune-for-sentiment-detection/IMDB Dataset.csv\")[:3000]  # Expect columns 'review' and 'sentiment'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:22:57.921784Z","iopub.execute_input":"2025-02-24T20:22:57.922009Z","iopub.status.idle":"2025-02-24T20:22:58.560434Z","shell.execute_reply.started":"2025-02-24T20:22:57.921990Z","shell.execute_reply":"2025-02-24T20:22:58.559723Z"}},"outputs":[],"execution_count":6},{"id":"128c147c-3e44-449e-b175-5d796b2bc066","cell_type":"code","source":"from datasets import load_dataset\ndf_summ = load_dataset(\"cnn_dailymail\", \"3.0.0\")\ndf_summ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:22:58.561225Z","iopub.execute_input":"2025-02-24T20:22:58.561433Z","iopub.status.idle":"2025-02-24T20:23:07.594177Z","shell.execute_reply.started":"2025-02-24T20:22:58.561415Z","shell.execute_reply":"2025-02-24T20:23:07.593448Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 287113\n    })\n    validation: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 13368\n    })\n    test: Dataset({\n        features: ['article', 'highlights', 'id'],\n        num_rows: 11490\n    })\n})"},"metadata":{}}],"execution_count":7},{"id":"db27563c-4180-41d8-b5d0-5d855af2aa83","cell_type":"code","source":"df_train_summ = df_summ['train'][:3000]\ndf_val_summ   = df_summ['validation'][:300]\ndf_test_summ  = df_summ['test'][:100]\ndf_train_summ = pd.concat([pd.Series(df_train_summ['article']),pd.Series(df_train_summ['highlights'])],axis=1).rename(columns={0: 'article', 1: 'highlights'})\ndf_val_summ = pd.concat([pd.Series(df_val_summ['article']),pd.Series(df_val_summ['highlights'])],axis=1).rename(columns={0: 'article', 1: 'highlights'})\ndf_test_summ = pd.concat([pd.Series(df_test_summ['article']),pd.Series(df_test_summ['highlights'])],axis=1).rename(columns={0: 'article', 1: 'highlights'})\ndf_train_summ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:23:07.595032Z","iopub.execute_input":"2025-02-24T20:23:07.595556Z","iopub.status.idle":"2025-02-24T20:23:07.613323Z","shell.execute_reply.started":"2025-02-24T20:23:07.595533Z","shell.execute_reply":"2025-02-24T20:23:07.612559Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                              article  \\\n0   LONDON, England (Reuters) -- Harry Potter star...   \n1   Editor's note: In our Behind the Scenes series...   \n2   MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...   \n3   WASHINGTON (CNN) -- Doctors removed five small...   \n4   (CNN)  -- The National Football League has ind...   \n..                                                ...   \n95  DENVER, Colorado -- A Colorado man terrorized ...   \n96  LONDON, England (CNN) -- Previously unseen foo...   \n97  WASHINGTON (CNN) -- Republicans reacted with s...   \n98  ST. PETERSBURG, Florida (CNN) -- The acrimony ...   \n99  BAGHDAD, Iraq (CNN) -- None of the 1,000-plus ...   \n\n                                           highlights  \n0   Harry Potter star Daniel Radcliffe gets £20M f...  \n1   Mentally ill inmates in Miami are housed on th...  \n2   NEW: \"I thought I was going to die,\" driver sa...  \n3   Five small polyps found during procedure; \"non...  \n4   NEW: NFL chief, Atlanta Falcons owner critical...  \n..                                                ...  \n95  Some witnesses say Colorado does nothing to pr...  \n96  NEW: Jury shown new footage of Diana taken hou...  \n97  Republican Sen. Lindsey Graham: \"I am astounde...  \n98  YouTube questions address taxes, the Bible, ab...  \n99  More than 1,000 freed detainees reportedly kee...  \n\n[100 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>article</th>\n      <th>highlights</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>LONDON, England (Reuters) -- Harry Potter star...</td>\n      <td>Harry Potter star Daniel Radcliffe gets £20M f...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Editor's note: In our Behind the Scenes series...</td>\n      <td>Mentally ill inmates in Miami are housed on th...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>MINNEAPOLIS, Minnesota (CNN) -- Drivers who we...</td>\n      <td>NEW: \"I thought I was going to die,\" driver sa...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>WASHINGTON (CNN) -- Doctors removed five small...</td>\n      <td>Five small polyps found during procedure; \"non...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>(CNN)  -- The National Football League has ind...</td>\n      <td>NEW: NFL chief, Atlanta Falcons owner critical...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>95</th>\n      <td>DENVER, Colorado -- A Colorado man terrorized ...</td>\n      <td>Some witnesses say Colorado does nothing to pr...</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>LONDON, England (CNN) -- Previously unseen foo...</td>\n      <td>NEW: Jury shown new footage of Diana taken hou...</td>\n    </tr>\n    <tr>\n      <th>97</th>\n      <td>WASHINGTON (CNN) -- Republicans reacted with s...</td>\n      <td>Republican Sen. Lindsey Graham: \"I am astounde...</td>\n    </tr>\n    <tr>\n      <th>98</th>\n      <td>ST. PETERSBURG, Florida (CNN) -- The acrimony ...</td>\n      <td>YouTube questions address taxes, the Bible, ab...</td>\n    </tr>\n    <tr>\n      <th>99</th>\n      <td>BAGHDAD, Iraq (CNN) -- None of the 1,000-plus ...</td>\n      <td>More than 1,000 freed detainees reportedly kee...</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":8},{"id":"556272c0-0781-466e-bb2f-acc9dea264c4","cell_type":"code","source":"def preprocess_texts(texts, max_length=128):\n    return tokenizer(texts.tolist(), padding=True, truncation=True,\n                     max_length=max_length, return_tensors=\"tf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:23:07.615967Z","iopub.execute_input":"2025-02-24T20:23:07.616198Z","iopub.status.idle":"2025-02-24T20:23:07.621716Z","shell.execute_reply.started":"2025-02-24T20:23:07.616178Z","shell.execute_reply":"2025-02-24T20:23:07.620809Z"}},"outputs":[],"execution_count":9},{"id":"6a902512-34bc-4608-91c0-fdbe7b763cfa","cell_type":"code","source":"emotion_inputs = preprocess_texts(emotion_df[\"text\"])\n# Convert emotion labels (assumed to be strings) to integer codes\nemotion_labels = tf.convert_to_tensor(emotion_df[\"label\"].astype('category').cat.codes.tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:23:07.622758Z","iopub.execute_input":"2025-02-24T20:23:07.622994Z","iopub.status.idle":"2025-02-24T20:23:07.709801Z","shell.execute_reply.started":"2025-02-24T20:23:07.622975Z","shell.execute_reply":"2025-02-24T20:23:07.709200Z"}},"outputs":[],"execution_count":10},{"id":"c7e8f87f-7917-42ac-b540-c2aeb5ae03b2","cell_type":"code","source":"sentiment_inputs = preprocess_texts(sentiment_df[\"review\"])\n# Convert sentiment labels to binary (e.g., \"positive\" -> 1, \"negative\" -> 0)\nsentiment_labels = tf.convert_to_tensor((sentiment_df[\"sentiment\"]==\"positive\").astype(int).tolist())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:23:07.710370Z","iopub.execute_input":"2025-02-24T20:23:07.710557Z","iopub.status.idle":"2025-02-24T20:23:08.422628Z","shell.execute_reply.started":"2025-02-24T20:23:07.710540Z","shell.execute_reply":"2025-02-24T20:23:08.421681Z"}},"outputs":[],"execution_count":11},{"id":"ddea0a16-38ae-41cf-85a4-84ca6064c43e","cell_type":"code","source":"def preprocess_summary_inputs(texts, max_input_length=512):\n    \"\"\"\n    Preprocess input texts for summarization by adding the 'summarize: ' prefix\n    and tokenizing.\n    \"\"\"\n    # Add the summarization prefix to each text\n    prefixed_texts = [\"summarize: \" + text for text in texts.tolist()]\n    return tokenizer(prefixed_texts, padding=\"max_length\", truncation=True,\n                     max_length=max_input_length, return_tensors=\"tf\")\n\ndef preprocess_summary_targets(targets, max_target_length=150):\n    \"\"\"\n    Preprocess target summaries by tokenizing.\n    \"\"\"\n    # Tokenize the target summaries; we only need the input_ids for targets.\n    tokenized_targets = tokenizer(targets.tolist(), padding=\"max_length\", truncation=True,\n                                  max_length=max_target_length, return_tensors=\"tf\")\n    return tokenized_targets[\"input_ids\"]\n\n# Example usage:\n# Assume summary_df is a DataFrame with columns \"text\" for source texts and \"summary\" for target summaries.\nsummary_inputs = preprocess_summary_inputs(df_train_summ[\"article\"])\nsummary_labels = preprocess_summary_targets(df_train_summ[\"highlights\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:23:08.423468Z","iopub.execute_input":"2025-02-24T20:23:08.423788Z","iopub.status.idle":"2025-02-24T20:23:08.797219Z","shell.execute_reply.started":"2025-02-24T20:23:08.423757Z","shell.execute_reply":"2025-02-24T20:23:08.796277Z"}},"outputs":[],"execution_count":12},{"id":"4a3faaca-1252-4a0f-bf06-35dcf1f3ea53","cell_type":"code","source":"# Create TensorFlow Datasets\ndef create_tf_dataset(inputs, labels, batch_size=8):\n    dataset = tf.data.Dataset.from_tensor_slices((dict(inputs), labels))\n    return dataset.shuffle(1000).batch(batch_size)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:23:08.798257Z","iopub.execute_input":"2025-02-24T20:23:08.798596Z","iopub.status.idle":"2025-02-24T20:23:08.802805Z","shell.execute_reply.started":"2025-02-24T20:23:08.798565Z","shell.execute_reply":"2025-02-24T20:23:08.801965Z"}},"outputs":[],"execution_count":13},{"id":"3697b9ae-0b75-444e-ac41-79c4be5d191d","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"51198317-3064-440a-b00b-729d01f3c13e","cell_type":"code","source":"emotion_dataset_tf = create_tf_dataset(emotion_inputs, emotion_labels)\nsentiment_dataset_tf = create_tf_dataset(sentiment_inputs, sentiment_labels)\nsummary_dataset_tf = create_tf_dataset(summary_inputs, summary_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:23:08.803667Z","iopub.execute_input":"2025-02-24T20:23:08.803938Z","iopub.status.idle":"2025-02-24T20:23:08.838423Z","shell.execute_reply.started":"2025-02-24T20:23:08.803917Z","shell.execute_reply":"2025-02-24T20:23:08.837575Z"}},"outputs":[],"execution_count":14},{"id":"7f184f9d-a5f4-4da8-8720-66bb8c302b41","cell_type":"code","source":"# Training setup\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n# optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n# optimizer.build(model.trainable_variables)  # Build optimizer with all model variables\noptimizer = tf.keras.optimizers.legacy.Adam(learning_rate=3e-5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:23:52.793075Z","iopub.execute_input":"2025-02-24T20:23:52.793392Z","iopub.status.idle":"2025-02-24T20:23:52.797718Z","shell.execute_reply.started":"2025-02-24T20:23:52.793368Z","shell.execute_reply":"2025-02-24T20:23:52.796688Z"}},"outputs":[],"execution_count":17},{"id":"5e8ded31-97d5-4b13-97c4-c09825463ccc","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"13101c52-3183-4327-a7c7-4e70cee224f9","cell_type":"code","source":"epochs = 2\nfor epoch in range(epochs):\n    print(f\"Epoch {epoch+1}\")\n\n    # Train on emotion task\n    for batch in emotion_dataset_tf:\n        inputs, labels = batch  # inputs: dict with 'input_ids', 'attention_mask'\n        with tf.GradientTape() as tape:\n            logits = model(inputs, task=\"emotion\", training=True)\n            loss = loss_fn(labels, logits)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    # Train on sentiment task\n    for batch in sentiment_dataset_tf:\n        inputs, labels = batch\n        with tf.GradientTape() as tape:\n            logits = model(inputs, task=\"sentiment\", training=True)\n            loss = loss_fn(labels, logits)\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n    \n    # Train on summarization task\n    for batch in summary_dataset_tf:\n        inputs, target_ids = batch  # target_ids: tokenized summary labels\n        with tf.GradientTape() as tape:\n            # Pass target_ids as labels so that decoder_input_ids are generated internally.\n            logits = model(inputs, task=\"summary\", training=True, labels=target_ids)\n            # Shift target_ids to compare with the logits.\n            # logits: (batch_size, target_seq_length, vocab_size)\n            # Use target_ids[:, 1:] as ground truth, and logits[:, :-1, :] for predictions.\n            loss = loss_fn(target_ids[:, 1:], logits[:, :-1, :])\n        gradients = tape.gradient(loss, model.trainable_variables)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n            \n    print(f\"Loss after epoch {epoch+1}: {loss.numpy()}\")\n","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:23:54.581193Z","iopub.execute_input":"2025-02-24T20:23:54.581526Z","iopub.status.idle":"2025-02-24T20:25:02.920235Z","shell.execute_reply.started":"2025-02-24T20:23:54.581487Z","shell.execute_reply":"2025-02-24T20:25:02.919449Z"}},"outputs":[{"name":"stdout","text":"Epoch 1\nLoss after epoch 1: 10.661019325256348\nEpoch 2\nLoss after epoch 2: 10.52542781829834\n","output_type":"stream"}],"execution_count":18},{"id":"dfe32fc2-c9ac-43fb-af87-40012d0e1af3","cell_type":"code","source":"# Save the model weights and tokenizer\nmodel.save_weights(\"./multi_task_T5_tf_weights\")\ntokenizer.save_pretrained(\"./multi_task_T5_tf\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:23:35.267684Z","iopub.status.idle":"2025-02-24T20:23:35.267969Z","shell.execute_reply":"2025-02-24T20:23:35.267839Z"}},"outputs":[],"execution_count":null},{"id":"fff0c104-3a43-46f2-9688-5850bfb8884f","cell_type":"code","source":"def predict(text, task):\n    if task in ['emotion', 'sentiment']:\n        # For classification tasks, tokenize with a moderate max_length.\n        inputs = tokenizer(text, return_tensors=\"tf\", max_length=128, truncation=True, padding=\"max_length\")\n        logits = model(inputs, task=task, training=False)\n        # Return the predicted label index.\n        return tf.argmax(logits, axis=1).numpy()\n    \n    elif task == 'summary':\n        # Prepend the summarization prefix as required by T5.\n        input_text = \"summarize: \" + text\n        # Use a longer max_length for summarization inputs.\n        inputs = tokenizer(input_text, return_tensors=\"tf\", max_length=512, truncation=True, padding=\"max_length\")\n        # Use the underlying T5 model's generate function.\n        generated_ids = model.t5.generate(\n            inputs[\"input_ids\"],\n            attention_mask=inputs[\"attention_mask\"],\n            max_length=150,\n            num_beams=4,\n            early_stopping=True\n        )\n        # Decode the generated token ids to text.\n        summary = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n        return summary\n    \n    else:\n        raise ValueError(\"Unsupported task type. Use 'emotion', 'sentiment', or 'summary'.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:25:13.036447Z","iopub.execute_input":"2025-02-24T20:25:13.037013Z","iopub.status.idle":"2025-02-24T20:25:13.042930Z","shell.execute_reply.started":"2025-02-24T20:25:13.036979Z","shell.execute_reply":"2025-02-24T20:25:13.041923Z"}},"outputs":[],"execution_count":19},{"id":"5649b336-c2cd-4676-b978-b684aeddf8bb","cell_type":"code","source":"# Define mapping dictionaries for sentiment and emotion tasks\nsentiment_mapping = {0: \"negative\", 1: \"positive\"}\nemotion_mapping = {\n    0: \"anger\", \n    1: \"fear\", \n    2: \"joy\", \n    3: \"love\", \n    4: \"sadness\", \n    5: \"surprise\"\n}\n\nprint(\"Sentiment mapping:\", sentiment_mapping)\nprint(\"Emotion mapping:\", emotion_mapping)\n\n# Function to decode a single sentiment label\ndef decode_sentiment(label_code):\n    return sentiment_mapping.get(label_code, \"Unknown\")\n\n# Function to decode a single emotion label\ndef decode_emotion(label_code):\n    return emotion_mapping.get(label_code, \"Unknown\")\n\n# Function to decode the summarization output (which is already a text string)\ndef decode_summary(summary_text):\n    return summary_text\n\n# Optionally, a unified decoding function for all tasks:\ndef decode_output(prediction, task):\n    if task == \"sentiment\":\n        # prediction should be a numeric label code\n        return decode_sentiment(int(prediction))\n    elif task == \"emotion\":\n        return decode_emotion(int(prediction))\n    elif task == \"summary\":\n        # prediction is the generated summary text\n        return decode_summary(prediction)\n    else:\n        return \"Unknown task\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:25:14.633291Z","iopub.execute_input":"2025-02-24T20:25:14.633620Z","iopub.status.idle":"2025-02-24T20:25:14.641253Z","shell.execute_reply.started":"2025-02-24T20:25:14.633591Z","shell.execute_reply":"2025-02-24T20:25:14.640306Z"}},"outputs":[{"name":"stdout","text":"Sentiment mapping: {0: 'negative', 1: 'positive'}\nEmotion mapping: {0: 'anger', 1: 'fear', 2: 'joy', 3: 'love', 4: 'sadness', 5: 'surprise'}\n","output_type":"stream"}],"execution_count":20},{"id":"ec0ca57e-b4b7-4aa7-8d34-58f03fde2e1d","cell_type":"code","source":"# Test texts for each task\nsentiment_text = \"I feel absolutely joyful and excited about the future!\"\nemotion_text = \"This makes me furious!\"\nsummary_text = (\"The rapid advances in technology over the past decade have \"\n                \"transformed how we interact with the world, leading to innovative \"\n                \"solutions across industries.\")\n\n# Get predictions from the model\nsentiment_pred = predict(sentiment_text, task=\"sentiment\")  # Returns a numpy array\nemotion_pred = predict(emotion_text, task=\"emotion\")          # Returns a numpy array\nsummary_pred = predict(summary_text, task=\"summary\")          # Returns a string\n\n# Decode predictions using our mapping functions\nprint(\"Sentiment:\", decode_sentiment(int(sentiment_pred[0])))\nprint(\"Emotion:\", decode_emotion(int(emotion_pred[0])))\nprint(\"Summary:\", summary_pred)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:25:16.597345Z","iopub.execute_input":"2025-02-24T20:25:16.597645Z","iopub.status.idle":"2025-02-24T20:25:23.295810Z","shell.execute_reply.started":"2025-02-24T20:25:16.597623Z","shell.execute_reply":"2025-02-24T20:25:23.295019Z"}},"outputs":[{"name":"stdout","text":"Sentiment: positive\nEmotion: joy\nSummary: rapid advances in technology over the past decade have transformed how we interact with the world.\n","output_type":"stream"}],"execution_count":21},{"id":"3505f167-f970-4f56-be15-d5c3217892af","cell_type":"code","source":"# Sample test datasets (lists of raw text) for each task\ntest_texts_sentiment = [\n    \"I feel absolutely joyful and excited about the future!\",\n    \"I am really disappointed with the service.\"\n]\n\ntest_texts_emotion = [\n    \"This makes me furious!\",\n    \"I feel so happy and content.\"\n]\n\ntest_texts_summary = [\n    \"The rapid advances in technology over the past decade have transformed the way we live, work, and communicate. These changes have led to groundbreaking innovations across industries.\",\n    \"Artificial intelligence is revolutionizing every sector, from healthcare to finance, by automating processes and providing new insights that were previously unimaginable.\"\n]\n\nprint(\"=== Sentiment Predictions ===\")\nfor text in test_texts_sentiment:\n    # Get prediction for sentiment task (expects a numpy array of label indices)\n    sentiment_pred = predict(text, task=\"sentiment\")\n    # Decode the first (and only) prediction in the batch\n    decoded_sentiment = decode_sentiment(int(sentiment_pred[0]))\n    print(f\"Text: {text}\")\n    print(f\"Predicted Sentiment: {decoded_sentiment}\\n\")\n\nprint(\"=== Emotion Predictions ===\")\nfor text in test_texts_emotion:\n    # Get prediction for emotion task (expects a numpy array of label indices)\n    emotion_pred = predict(text, task=\"emotion\")\n    # Decode the first prediction in the batch\n    decoded_emotion = decode_emotion(int(emotion_pred[0]))\n    print(f\"Text: {text}\")\n    print(f\"Predicted Emotion: {decoded_emotion}\\n\")\n\nprint(\"=== Summarization Predictions ===\")\nfor text in test_texts_summary:\n    # For summarization, predict returns a generated summary text string.\n    summary = predict(text, task=\"summary\")\n    print(f\"Original Text: {text}\")\n    print(f\"Generated Summary: {summary}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:26:06.381626Z","iopub.execute_input":"2025-02-24T20:26:06.382033Z","iopub.status.idle":"2025-02-24T20:26:13.781019Z","shell.execute_reply.started":"2025-02-24T20:26:06.382001Z","shell.execute_reply":"2025-02-24T20:26:13.780167Z"}},"outputs":[{"name":"stdout","text":"=== Sentiment Predictions ===\nText: I feel absolutely joyful and excited about the future!\nPredicted Sentiment: positive\n\nText: I am really disappointed with the service.\nPredicted Sentiment: positive\n\n=== Emotion Predictions ===\nText: This makes me furious!\nPredicted Emotion: joy\n\nText: I feel so happy and content.\nPredicted Emotion: joy\n\n=== Summarization Predictions ===\nOriginal Text: The rapid advances in technology over the past decade have transformed the way we live, work, and communicate. These changes have led to groundbreaking innovations across industries.\nGenerated Summary: rapid advances in technology over the past decade have transformed the way we live, work, and communicate.\n\nOriginal Text: Artificial intelligence is revolutionizing every sector, from healthcare to finance, by automating processes and providing new insights that were previously unimaginable.\nGenerated Summary: artificial intelligence is revolutionizing every sector, from healthcare to finance, by automating processes and providing new insights.\n\n","output_type":"stream"}],"execution_count":22},{"id":"9738bca4-a44d-401a-a443-eddd213466f9","cell_type":"code","source":"# from transformers import T5Tokenizer\n\n# 1. Save T5 encoder-decoder component\nmodel.t5.save_pretrained('multitask_t5')\n\n# 2. Save classification heads (assuming they are grouped into one tf.keras.Model)\ntf.keras.models.save_model(model.classification_heads, 'classification_heads')\n\n# 3. Save tokenizer (here using t5-large as the base)\ntokenizer = T5Tokenizer.from_pretrained('t5-small')\ntokenizer.save_pretrained('saved_t5_tokenizer')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-24T20:28:08.825899Z","iopub.execute_input":"2025-02-24T20:28:08.826279Z","iopub.status.idle":"2025-02-24T20:28:09.481095Z","shell.execute_reply.started":"2025-02-24T20:28:08.826248Z","shell.execute_reply":"2025-02-24T20:28:09.479746Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3805\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3806\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mindex.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'text'","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-75a8cb3469e8>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Convert the 'text' column to lists of texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtest_texts_sentiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentiment_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtest_texts_emotion\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0memotion_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtest_texts_summary\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_summary_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummary_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4101\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4102\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4103\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4104\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3810\u001b[0m             ):\n\u001b[1;32m   3811\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mInvalidIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3812\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3813\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3814\u001b[0m             \u001b[0;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'text'"],"ename":"KeyError","evalue":"'text'","output_type":"error"}],"execution_count":24},{"id":"36faa991-be55-417f-84b6-9f494bb86c40","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"448ed49e-ecf2-47e4-ba95-19fdaccbee82","cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}